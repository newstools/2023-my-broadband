A Reddit user has claimed that the highly-detailed Moon photos that Samsung’s top-end Galaxy S smartphones can take are heavily computer-generated instead of using the built-in camera capabilities that Samsung claims they do. Samsung first introduced its 100x Space Zoom feature with the Galaxy S20 Ultra in 2020, allowing owners to take stunning images of the Earth’s Moon, around 382,500 kilometres away. But a Redit user who goes by “ibreakphotos” said the Moon pictures from Samsung were “fake” and accused the company of deceptive marketing in its descriptions of the capability. “They mention multi-frames [and] multi-exposures, but the reality is, it’s AI doing most of the work, not the optics. The optics aren’t capable of resolving the detail that you see,” he stated. His findings are based on an experiment that fooled the S20 Ultra’s camera into thinking it was taking a picture of the actual Moon, while it was actually capturing a low-res image of the Moon on a computer monitor in a dark room. Ibreakphotos had downloaded a high-resolution image of the Moon from the Internet and significantly reduced its quality by downscaling it to a 170 x 170 resolution and adding a gaussian blur. He then put the low-res image up on his computer monitor, switched off the lights in the room and stood far enough to zoom in at 100x and take the picture. The resulting image showed the camera had added craters and other details that were completely missing in the original image. That proved it had either completely made up the additional details or had cross-referenced the angle of the Moon in the original picture with its own dataset of images and added the details in the latter to the photo. “None of the frames have the craters because they’re intentionally blurred, yet the camera somehow miraculously knows that they are there,” ibreakphotos said. Ibreakphotos emphasised that what the camera was doing was different from the kind of processing performed when zooming into other objects. That processing use multiple exposures and combined data from several frames to recover detail that a single frame might miss, a common feature of many high-end smartphone cameras sometimes referred to as “super-resolution”. He explained that the issue was specific to the Moon as it was an object on which Samsung could easily train a generative machine learning model. “Since the Moon is tidally locked to the Earth, it’s very easy to train your model on other moon images and just slap that texture when a Moon-like thing is detected,” he said. “It’s not sharpening, it’s not adding detail from multiple frames because in this experiment, all the frames contain the same amount of detail.” Another Reddit user pointed out that Samsung had actually acknowledged the use of artificial intelligence (AI) in Moon photos on recent Galaxy S smartphones in a CamCyclopedia article. “The Moon recognition engine was created by learning various Moon shapes from full Moon to crescent Moon, based on images that people actually see with their eyes on Earth,” the author explained. “It uses an AI deep learning model to show the presence of the Moon in the image and the area (square box) as a result.” “AI models that have been trained can detect lunar areas even if other lunar images that have not been used for training are inserted.” The Huawei P30 Pro previously caused similar controversy, after a researcher found its Moon Mode was merely superimposing existing photos of the Moon from a database. Samsung’s approach seems to be less on the nose but nonetheless relies heavily on computer-based generation rather than the camera’s sensor or super-resolution software.