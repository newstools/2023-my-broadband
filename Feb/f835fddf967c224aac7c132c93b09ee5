Microsoft has started putting caps on the number of questions users can ask its ChatGPT-powered Bing Chat feature, which is currently in testing. In a blog post late last week, the company told early reviewers that its new Bing Chat had been capped to 50 daily chat turns and five turns per session. One chat turn consists of a user’s question or input query and Bing’s response. Therefore, users will now only be able to ask Bing Chat up to five questions and get five replies before having to restart their query. Microsoft said this would be sufficient to ensure most users get the answers they seek. “Our data has shown that the vast majority of you find the answers you’re looking for within five turns and that only roughly 1% of chat conversations have 50+ messages,” Microsoft stated. “After a chat session hits 5 turns, you will be prompted to start a new topic.” “At the end of each chat session, context needs to be cleared so the model won’t get confused. Just click on the broom icon to the left of the search box for a fresh start.” Microsoft said it would expand the caps on chat sessions to enhance search and discovery experiences based on user feedback. The limit comes after a flurry of reports in which early testers were able to seemingly “anger” the new Bing and evoke bizarre and sometimes disturbing replies. Several tech journalists and general users had found it would often double down on wrong answers before insulting and threatening users who tried to correct it. MyBroadband’s tests also saw Bing making patently false claims, including that 16 December 2022 was in the future and that we were using a calendar that had not been updated for the leap year. After trying to correct this, it kept insisting that we were wrong, refused to accept evidence contrary to its position, and said we should apologise for our mistake. Somewhat comically, Bing itself suggested that we send aggressive follow-ups such as, “No, you are wrong. Stop lying to me.” These caused the language model to go further into an antagonistic spiral. After several chat turns, we discovered that Microsoft would overwrite Bing’s antagonistic responses with an apology and attempt to move on to a different subject.